---
title: Message Delivery Semantics
description: Message Delivery Semantics, Exactly Once and Beyond
date: 2026-01-01
---

# Message Delivery Semantics

In distributed systems, ensuring reliable message delivery is one of the most challenging problems to solve. Network failures, broker crashes, and application restarts can all conspire to lose or duplicate your data. This post explores Kafka's delivery guaranteesâ€”from the simple at-most-once to the sophisticated exactly-once semanticsâ€”and how to extend these guarantees when integrating with external systems.

## The Three Delivery Guarantees

Before diving into implementation, let's establish what each guarantee actually means:

| Semantic      | Behavior               | Trade-off          | Use Case             |
| ------------- | ---------------------- | ------------------ | -------------------- |
| At-Most-Once  | Fire and forget        | May lose messages  | Metrics, logging     |
| At-Least-Once | Retry on failure       | May duplicate      | Default production   |
| Exactly-Once  | No loss, no duplicates | Highest complexity | Financial, inventory |

### At-Most-Once: Speed Over Safety

The producer sends a message and moves on without waiting for acknowledgment. If the broker fails to receive it, the message is lost forever.

```java
Properties config = new Properties();
config.put("bootstrap.servers", "localhost:9092");
config.put("acks", "0");        // Don't wait for any acknowledgment
config.put("retries", "0");     // Never retry
```

### At-Least-Once: Safety Over Simplicity

The producer retries until it receives acknowledgment. This guarantees delivery but may result in duplicates if the broker wrote the message but the acknowledgment was lost.

```java
Properties config = new Properties();
config.put("bootstrap.servers", "localhost:9092");
config.put("acks", "all");                       // Wait for all replicas
config.put("retries", Integer.MAX_VALUE);
config.put("enable.idempotence", "false");       // Duplicates possible
```

### Exactly-Once: The Gold Standard

Each message is delivered and processed exactly once, even when failures occur. Kafka achieves this through idempotent producers, transactions, and transactional consumers working together.

---

## Idempotent Producers: Eliminating Duplicates

Idempotence ensures that even if the producer retries sending a message, it's written exactly once to the log. This is the foundation of Kafka's exactly-once semantics.

### How It Works

When `enable.idempotence=true`, each producer receives a unique **Producer ID (PID)** and assigns monotonically increasing **sequence numbers** per partition:

```
Producer (PID=1000)                    Broker (Partition 0)
      â”‚                                      â”‚
      â”‚â”€â”€ ProduceRequest â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
      â”‚   (pid=1000, seq=0, "order-1")       â”‚
      â”‚                                      â”‚ Writes record, tracks seq=0
      â”‚â—„â”€â”€ ACK (success) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
      â”‚                                      â”‚
      â”‚â”€â”€ ProduceRequest â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
      â”‚   (pid=1000, seq=1, "order-2")       â”‚
      â”‚                                      â”‚ Writes record, tracks seq=1
      â”‚         âœ— ACK lost (network issue)   â”‚
      â”‚                                      â”‚
      â”‚â”€â”€ ProduceRequest (retry) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
      â”‚   (pid=1000, seq=1, "order-2")       â”‚
      â”‚                                      â”‚ seq=1 already seen â†’ deduplicate!
      â”‚â—„â”€â”€ ACK (success, deduplicated) â”€â”€â”€â”€â”€â”€â”‚
```

The broker maintains a small buffer tracking the last 5 sequence numbers per PID/partition pair. Any message with a sequence number already seen is acknowledged but not written again.

### Configuration (Kafka 4.x Defaults)

Since Kafka 3.0, idempotence is enabled by default:

| Config                                  | Default   | Purpose                                |
| --------------------------------------- | --------- | -------------------------------------- |
| `enable.idempotence`                    | `true`    | Enable PID/sequence tracking           |
| `acks`                                  | `all`     | Wait for all ISR to acknowledge        |
| `retries`                               | `MAX_INT` | Retry indefinitely on transient errors |
| `max.in.flight.requests.per.connection` | `5`       | Max unacknowledged batches             |

With idempotence enabled and `max.in.flight.requests.per.connection â‰¤ 5`, Kafka guarantees ordering even with retries.

### Limitations of Idempotence Alone

Idempotence provides exactly-once within a single partition for a single producer session. It does **not** help with:

- Atomic writes across multiple partitions or topics
- Coordinating consumer offset commits with produced messages
- Producer crashes and restarts (new PID = new sequence space)

For these scenarios, you need transactions.

### Why Offset Coordination Matters

In a consume-transform-produce workflow, you have two operations that must succeed together:

1. **Produce** the result to an output topic
2. **Commit** the consumer offset to mark input as processed

Without transactions, these are independent and can fail independently:

```
Consumer                                         Broker
    â”‚                                               â”‚
    â”‚â—„â”€â”€ poll() returns msg at offset 10 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
    â”‚                                               â”‚
    â”‚    [process message]                          â”‚
    â”‚                                               â”‚
    â”‚â”€â”€ produce result to output topic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ âœ“ Written
    â”‚                                               â”‚
    â”‚â”€â”€ commit offset 11 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
    â”‚         ðŸ’¥ CRASH before commit!               â”‚
```

On restart, the consumer asks "what's my last committed offset?" and gets back **10** (since 11 was never committed). It re-reads the same message, processes it again, and produces another copy to the output topic.

**Why doesn't idempotent producer help?** Because you get a **new PID** on restart:

```
Before crash:  PID=1000, seq=42, payload="result-A"  â†’ Written
After restart: PID=1001, seq=0,  payload="result-A"  â†’ Written again!
```

The broker sees a completely different producerâ€”it has no way to know this is a retry of the same logical operation. Idempotence only deduplicates within the same producer session.

**Transactions solve this** by making both operations atomic:

```java
producer.beginTransaction();
producer.send(new ProducerRecord<>("output", key, result));
producer.sendOffsetsToTransaction(offsets, consumer.groupMetadata());
producer.commitTransaction();  // Both succeed or both fail
```

Now only two outcomes are possible:

| Scenario            | Output Message                 | Offset        | On Restart                   |
| ------------------- | ------------------------------ | ------------- | ---------------------------- |
| Commit succeeds     | Visible                        | Updated to 11 | Resume from 11, no reprocess |
| Crash before commit | Not visible (`read_committed`) | Still 10      | Reprocess, but no duplicate  |

The uncommitted message from the failed transaction is filtered out for `read_committed` consumersâ€”so even though you reprocess and produce again, only one copy is ever visible.

---

## Kafka Transactions: Atomic Multi-Partition Writes

Transactions extend exactly-once guarantees to atomic writes across multiple partitions and topics. They enable the consume-transform-produce pattern that powers exactly-once stream processing.

### Transaction Lifecycle

```
initTransactions()      Called once at startup
       â”‚                â€¢ Registers transactional.id with coordinator
       â”‚                â€¢ Aborts any pending transactions from zombies
       â”‚                â€¢ Bumps epoch to fence old producers
       â–¼
beginTransaction()      Starts a new transaction (local state only)
       â”‚
       â–¼
send() / send()         Produce messages (buffered, marked transactional)
       â”‚
       â–¼
sendOffsetsToTransaction()   Commit consumer offsets atomically
       â”‚
       â–¼
commitTransaction()     Two-phase commit
       â”‚                â€¢ Writes PREPARE_COMMIT to coordinator
       â”‚                â€¢ Writes COMMIT markers to all partitions
       â”‚                â€¢ Writes COMPLETE_COMMIT to coordinator
       â–¼
[Complete]              Messages now visible to read_committed consumers
```

### The Transaction Coordinator

Every transactional producer is assigned a **transaction coordinator**â€”a broker responsible for managing that producer's transaction state. The coordinator:

1. Maintains state in the `__transaction_state` internal topic (50 partitions)
2. Assigns and tracks producer epochs for zombie fencing
3. Orchestrates the two-phase commit protocol
4. Writes commit/abort markers to participating partitions

Coordinator assignment: `hash(transactional.id) % 50`

### Transactional Producer Example

```java
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("transactional.id", "order-processor-1");
props.put("key.serializer", StringSerializer.class.getName());
props.put("value.serializer", StringSerializer.class.getName());

KafkaProducer<String, String> producer = new KafkaProducer<>(props);

// Initialize once at startup - registers with coordinator, fences zombies
producer.initTransactions();

try {
    producer.beginTransaction();

    producer.send(new ProducerRecord<>("orders", "order-123", "{\"status\":\"confirmed\"}"));
    producer.send(new ProducerRecord<>("inventory", "sku-456", "{\"reserved\":5}"));
    producer.send(new ProducerRecord<>("notifications", "user-789", "{\"type\":\"confirmed\"}"));

    producer.commitTransaction();
} catch (ProducerFencedException e) {
    // Another producer with same transactional.id is active
    producer.close();
} catch (KafkaException e) {
    producer.abortTransaction();
}
```

---

## Zombie Fencing: Preventing Split-Brain

A "zombie" is a process that appears dead but is actually still runningâ€”perhaps due to a network partition or long GC pause. Zombies can cause duplicate processing if not properly handled.

### How Fencing Works

Each `transactional.id` has an associated **epoch** stored by the transaction coordinator. When a new producer calls `initTransactions()`:

1. The coordinator increments the epoch
2. Any in-progress transactions from the old epoch are aborted
3. The new producer receives the updated epoch
4. Any requests from producers with older epochs are rejected with `ProducerFencedException`

```
Timeline:

Producer A (txn-id: "order-proc", epoch=0)
    â”‚â”€â”€ beginTransaction()
    â”‚â”€â”€ send(msg1)
    â”‚
    â”‚   [Network partition - A appears dead]
    â”‚
    â”‚   Producer B starts (same txn-id)
    â”‚   â”‚â”€â”€ initTransactions() â†’ epoch bumped to 1
    â”‚   â”‚â”€â”€ beginTransaction()
    â”‚   â”‚â”€â”€ send(msg2)
    â”‚   â”‚â”€â”€ commitTransaction() âœ“
    â”‚
    â”‚   [A recovers]
    â”‚
    â”‚â”€â”€ send(msg3) â†’ FENCED! ProducerFencedException (epoch=0 < 1)
```

### Transactional ID Strategy

**Pre-Kafka 2.5**: Required one transactional.id per input partition

```
transactional.id = "app-" + groupId + "-" + topic + "-" + partition
```

**Kafka 2.5+ (exactly_once_v2)**: Consumer group metadata enables proper fencing with a single producer

```
transactional.id = "app-" + instanceId
```

---

## Consumer Isolation Levels

The `isolation.level` configuration controls what consumers see:

| Level              | Behavior                                                 |
| ------------------ | -------------------------------------------------------- |
| `read_uncommitted` | See all messages including uncommitted/aborted (default) |
| `read_committed`   | Only see committed transactional messages                |

### Last Stable Offset (LSO)

The **Last Stable Offset (LSO)** is the offset of the first message that belongs to an open (undecided) transaction. A `read_committed` consumer can only fetch messages **before** the LSOâ€”everything before it is "stable" (either non-transactional or from a decided transaction).

```
Partition Log:

Offset  Message              Transaction    State
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  0     msg-A                (none)         Decided (non-txn)
  1     msg-B                txn-1          Decided (committed)
  2     msg-C                txn-1          Decided (committed)
  3     COMMIT marker        txn-1
  4     msg-D                (none)         Decided (non-txn)
  5     msg-E                txn-2          UNDECIDED â—„â”€â”€â”€ LSO
  6     msg-F                txn-2          UNDECIDED
  7     msg-G                (none)         Decided (but blocked!)
  8     msg-H                txn-2          UNDECIDED
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                                            LEO = 9, LSO = 5

read_committed consumer can only fetch offsets 0-4
```

Notice: **msg-G at offset 7 is non-transactional** (immediately decided), but the consumer cannot read it yet. The broker enforces the LSO limit at fetch timeâ€”undecided messages are never sent to `read_committed` consumers:

```
Consumer (read_committed)                    Broker
    â”‚                                           â”‚
    â”‚â”€â”€ FetchRequest(offset=5) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
    â”‚                                           â”‚ LSO=5, nothing safe to send
    â”‚â—„â”€â”€ FetchResponse(empty) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
    â”‚                                           â”‚
    â”‚   [blocked, waiting...]                   â”‚
    â”‚                                           â”‚
    â”‚                                           â”‚ txn-2 commits, LSO â†’ 10
    â”‚â”€â”€ FetchRequest(offset=5) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
    â”‚â—„â”€â”€ FetchResponse(offsets 5-9) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ Now safe to send
```

### Long Transactions Block All Consumers

If a producer starts a transaction and takes a long time to commit (or crashes), the LSO stays stuck and blocks all `read_committed` consumersâ€”even from reading non-transactional messages that arrived later:

```
Timeline:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º

Producer A: beginTransaction()
            â”‚â”€â”€ send(msg-E, offset 5)
            â”‚
            â”‚   [hangs or goes slow...]
            â”‚
            â”‚                    Meanwhile, other producers write:
            â”‚                      msg-G (offset 7), msg-I (offset 9)...
            â”‚
            â”‚   LSO stuck at 5!
            â”‚   Consumers blocked from ALL messages at offset â‰¥ 5
            â”‚
            â”‚â”€â”€ commitTransaction()
            â”‚
                 LSO advances, consumers catch up
```

This is why `transaction.timeout.ms` exists (default 60s)â€”the broker automatically aborts transactions that exceed this duration, allowing the LSO to advance.

### How Aborted Messages Are Filtered

When a transaction aborts, the LSO advances and the broker sends all messages (including aborted ones) to the consumer. The consumer then filters out aborted messages client-side:

```
After txn-2 aborts, LSO advances to 10:

Broker sends:     [msg-E] [msg-F] [msg-G] [msg-H] [ABORT marker]
                   (txn-2) (txn-2) (none)  (txn-2)

Consumer filters: [msg-E] [msg-F]         [msg-H]  â† skipped (aborted)

Consumer delivers only: [msg-G]
```

| Stage                      | What Happens                                         |
| -------------------------- | ---------------------------------------------------- |
| Fetch (broker-side)        | Limits response to LSOâ€”undecided messages never sent |
| After abort (client-side)  | Filters out aborted messages based on abort marker   |
| After commit (client-side) | Delivers all messages normally                       |

### Consumer Configuration

```java
Properties config = new Properties();
config.put("bootstrap.servers", "localhost:9092");
config.put("group.id", "order-consumers");
config.put("isolation.level", "read_committed");
config.put("enable.auto.commit", "false");
config.put("key.deserializer", StringDeserializer.class.getName());
config.put("value.deserializer", StringDeserializer.class.getName());
```

---

## The Consume-Transform-Produce Pattern

This pattern is the foundation of exactly-once stream processing: consume from input topics, process, and produce to output topicsâ€”all atomically.

```java
KafkaConsumer<String, String> consumer = new KafkaConsumer<>(consumerProps);
KafkaProducer<String, String> producer = new KafkaProducer<>(producerProps);

producer.initTransactions();
consumer.subscribe(List.of("orders"));

while (true) {
    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));

    if (!records.isEmpty()) {
        producer.beginTransaction();
        try {
            // Process and produce
            for (ConsumerRecord<String, String> record : records) {
                String result = process(record.value());
                producer.send(new ProducerRecord<>("results", record.key(), result));
            }

            // Build offsets to commit
            Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();
            for (TopicPartition partition : records.partitions()) {
                var partitionRecords = records.records(partition);
                long lastOffset = partitionRecords.get(partitionRecords.size() - 1).offset();
                offsets.put(partition, new OffsetAndMetadata(lastOffset + 1));
            }

            // Commit offsets atomically with produced messages
            producer.sendOffsetsToTransaction(offsets, consumer.groupMetadata());
            producer.commitTransaction();
        } catch (Exception e) {
            producer.abortTransaction();
        }
    }
}
```

The key: `sendOffsetsToTransaction` commits consumer offsets **within the same transaction** as produced messages. If the transaction aborts, offsets aren't committed and the consumer re-reads the same messages.

---

## Exactly-Once with External Systems

Kafka's exactly-once guarantees apply only within Kafka. When writing to databases or calling APIs, you face the **dual-write problem**: two systems that can't share a transaction boundary.

### Pattern 1: Idempotent Consumer

Make your consumer logic idempotent so processing the same message multiple times has the same effect as processing it once.

**Strategy A: Database Upserts**

```sql
INSERT INTO orders (order_id, status, amount)
VALUES ($1, $2, $3)
ON CONFLICT (order_id) DO UPDATE SET status = $2, amount = $3;
```

**Strategy B: Deduplication Table**

```java
void processMessage(ConsumerRecord<String, String> record) {
    String messageId = new String(record.headers().lastHeader("message-id").value());

    db.transaction(() -> {
        if (!db.exists("processed_messages", messageId)) {
            Result result = transform(record.value());
            db.insert("results", result);
            db.insert("processed_messages", messageId, Instant.now());
        }
    });
}
```

**Strategy C: Idempotency Keys for APIs**

```java
HttpRequest request = HttpRequest.newBuilder()
    .uri(URI.create("https://payment.service/charge"))
    .header("Idempotency-Key", record.key())
    .POST(HttpRequest.BodyPublishers.ofString(body))
    .build();
```

### Pattern 2: Transactional Outbox

Instead of writing to both database and Kafka, write only to the databaseâ€”including an "outbox" table for events to publish.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Single Database Transaction             â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ orders      â”‚      â”‚ outbox_events    â”‚     â”‚
â”‚  â”‚ (business)  â”‚      â”‚ (to publish)     â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚              COMMIT (atomic)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
            CDC (Debezium) / Poller
                      â”‚
                      â–¼
                Kafka Topic
```

```java
db.transaction(() -> {
    // Business logic
    db.update("orders", orderId, Map.of("status", "confirmed"));

    // Write to outbox (same transaction)
    db.insert("outbox_events", Map.of(
        "event_id", UUID.randomUUID(),
        "aggregate_type", "Order",
        "aggregate_id", orderId,
        "event_type", "OrderConfirmed",
        "payload", "{\"orderId\":\"" + orderId + "\",\"status\":\"confirmed\"}",
        "created_at", Instant.now()
    ));
});
```

| Publishing Approach | Pros                   | Cons                  |
| ------------------- | ---------------------- | --------------------- |
| Polling             | Simple, no extra infra | Latency, DB load      |
| Debezium CDC        | Low latency, ordering  | Additional complexity |
| Kafka Connect JDBC  | Configuration-driven   | Polling-based         |

---

## Transaction Performance

Transactions add overhead. Understanding where helps you optimize:

### Write Amplification

Each transaction adds:

- RPCs to register partitions with coordinator
- `PREPARE_COMMIT` record to `__transaction_state`
- `COMMIT` marker to each participating partition
- `COMPLETE_COMMIT` record to `__transaction_state`

### Latency Impact

| Commit Interval | Overhead | Use Case          |
| --------------- | -------- | ----------------- |
| Per-message     | ~50%     | Ultra-low latency |
| 100ms           | ~10%     | Balanced          |
| 1000ms          | ~2%      | High throughput   |

### Best Practices

```java
// Good: batch multiple records per transaction
producer.beginTransaction();
for (ProducerRecord<String, String> record : batch) {
    producer.send(record);
}
producer.commitTransaction();

// Bad: one transaction per record (high overhead)
for (ProducerRecord<String, String> record : records) {
    producer.beginTransaction();
    producer.send(record);
    producer.commitTransaction();
}
```

Additional recommendations:

- Keep transactions short to avoid delaying LSO
- Set `transaction.timeout.ms` to exceed your longest expected transaction
- Monitor for hung transactions using `kafka-transactions.sh`

---

## Kafka Streams: EOS Made Easy

Kafka Streams handles all exactly-once complexity for you:

```java
Properties props = new Properties();
props.put(StreamsConfig.APPLICATION_ID_CONFIG, "order-processor");
props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
props.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, "exactly_once_v2");
props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());

StreamsBuilder builder = new StreamsBuilder();
builder.<String, String>stream("orders")
    .mapValues(order -> processOrder(order))
    .to("processed-orders");

KafkaStreams streams = new KafkaStreams(builder.build(), props);
Runtime.getRuntime().addShutdownHook(new Thread(streams::close));
streams.start();
```

With `exactly_once_v2` (Kafka 2.5+):

- Each task uses transactions internally
- Consumer offsets committed atomically with output
- Zombie fencing via consumer group metadata
- One producer per thread (not per partition)

---

## Choosing the Right Guarantee

| Scenario                  | Recommendation                               |
| ------------------------- | -------------------------------------------- |
| Metrics, logging          | At-most-once (`acks=0`)                      |
| General messaging         | At-least-once (default idempotent producer)  |
| Kafka-to-Kafka processing | Exactly-once (transactions or Kafka Streams) |
| Kafka-to-database         | Idempotent consumer + transactional outbox   |
| Kafka-to-external API     | Idempotent consumer + idempotency keys       |

---

## Key Takeaways

1. **Idempotent producers are now default**: Since Kafka 3.0, you get exactly-once within partitions automatically.

2. **Transactions enable atomic multi-partition writes**: Use them for consume-transform-produce patterns.

3. **Zombie fencing is automatic**: The `transactional.id` + epoch mechanism prevents duplicate processing from crashed producers.

4. **`read_committed` is required**: Consumers must opt-in to see only committed transactional messages.

5. **External systems need additional patterns**: The outbox pattern and idempotent consumers extend exactly-once beyond Kafka.

6. **Kafka Streams simplifies everything**: For stream processing, `exactly_once_v2` handles all the complexity.

## References

- [Kafka Documentation: Message Delivery Semantics](https://kafka.apache.org/documentation/#semantics)
- [Exactly-once Semantics is Possible: Here's How Kafka Does It](https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it/)
- [Transactions in Apache Kafka](https://www.confluent.io/blog/transactions-apache-kafka/)
- [KIP-447: Producer Scalability for Exactly Once Semantics](https://cwiki.apache.org/confluence/display/KAFKA/KIP-447)
